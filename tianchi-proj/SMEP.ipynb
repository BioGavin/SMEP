{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 一、数据处理\n\n本小节主要对原始数据进行预处理，包括数据清洗、数据格式化、特征抽取等，从而获得用于训练的输入数据","metadata":{}},{"cell_type":"code","source":"import time\nimport os\nimport pandas as pd\nimport math\nfrom featured_data_generated import cal_pep_des_multiprocess\n\nimport warnings\nwarnings.simplefilter(\"ignore\", (UserWarning, FutureWarning))","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef timer(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"{func.__name__} 执行时间: {end_time - start_time:.4f} 秒\")\n        return result\n    return wrapper","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class GenerateSample():\n    \"\"\"用于生成蛋白质或肽的分类与回归样本。\n\n    该类从给定的正样本和负样本数据集中筛选、处理数据，并生成用于机器学习的分类和回归数据集。\n\n    Attributes:\n        gramapa_path (str): 包含正样本数据的文件路径。\n        negative_file_path (str): 包含负样本数据的文件路径。\n        generate_example_path (str): 生成样本数据的输出路径。\n        mode (str): 数据生成模式，'all' 表示包含所有样本，其他值仅生成正样本。\n    \"\"\"\n\n    def __init__(self, gramapa_path, negative_file_path, generate_example_path, mode):\n        \"\"\"初始化 GenerateSample 类。\n\n        Args:\n            grampa_path (str): 正样本数据文件路径。\n            negative_file_path (str): 负样本数据文件路径。\n            generate_example_path (str): 输出数据存储路径。\n            mode (str): 数据生成模式，'all' 表示包含所有样本，其他值表示仅处理正样本。\n        \"\"\"\n        self.gramapa_path = gramapa_path\n        self.negative_file_path = negative_file_path\n        self.generate_example_path = generate_example_path\n        self.mode = mode\n\n    @timer\n    def __call__(self, *args, **kwargs):\n        \"\"\"调用该类实例，执行样本生成流程。\n\n        处理步骤包括：\n        1. 读取并筛选正样本数据\n        2. 生成所有肽序列的描述符\n        3. 读取负样本数据并进行处理\n        4. 合并正负样本数据\n        5. 生成分类和回归样本数据，并保存到文件\n        \"\"\"\n        \n        positive_sample = self.generate_positive_peptides(self.gramapa_path)\n        negtive_sample = self.generate_negative_data(self.negative_file_path)\n        all_sample = self.concat_datasets(positive_sample, negtive_sample)\n        \n        print(\"generating xgboost sample.....\")\n        num = len(all_sample)\n        start = time.time()\n        sequence = all_sample[\"sequence\"]\n        peptides = sequence.values.copy().tolist()\n        result = all_sample[\"MIC\"]\n        type = all_sample[\"type\"]\n        xgb_sample_df = cal_pep_des_multiprocess.cal_pep_parallel(peptides, sequence, result, type)\n        self.split_sample(xgb_sample_df, \"xgb\")\n        end = time.time()\n        print(\"generate xgboost feature data cost time:\", (end - start) / num)\n\n        print(\"generating lstm sample.....\")\n        if self.mode == \"all\":\n            self.split_sample(all_sample, \"lstm\")\n        else:\n            self.split_sample(positive_sample, \"lstm\")\n        print(\"lstm sample is ok\")\n\n    def filter_data_with_str(self, col_name, str, data):\n        \"\"\"筛选包含特定字符串的行。\n\n        Args:\n            col_name (str): 要筛选的列名。\n            str_value (str): 要匹配的字符串。\n            data (pd.DataFrame): 输入数据。\n\n        Returns:\n            pd.DataFrame: 过滤后的数据集。\n        \"\"\"\n        bool_filter = data[col_name].str.contains(str)\n        filter_data = data[bool_filter]\n        return filter_data\n\n    def positive_data_filter(self, data):\n        \"\"\"筛选符合标准的正样本数据。\n\n        过滤规则：\n        1. 仅保留 C 端酰胺化（C-terminal amidation）的序列\n        2. 仅保留序列长度在 5 到 50 之间的样本\n\n        Args:\n            data (pd.DataFrame): 输入正样本数据。\n\n        Returns:\n            pd.DataFrame: 过滤后的正样本数据。\n        \"\"\"\n        data = data[data[\"has_cterminal_amidation\"] == True]\n        data = data[data[\"sequence\"].apply(lambda x: 5 < len(x) <= 50)]\n        return data\n\n    def generate_positive_peptides(self, positive_file_path):\n        \"\"\"计算所有肽序列的 MIC 值，并标记为正样本。\n        \n        该方法的核心流程如下：\n        1. 获取 `sequence` 列中所有唯一的肽序列。\n        2. 对于每个唯一肽序列：\n            - 计算其 MIC 值的 10 次方之和（用于后续计算几何平均值）。\n            - 计算该肽序列在数据集中出现的次数（用于取平均）。\n            - 计算其平均 MIC 值，并标记该肽序列为正样本（type = 1）。\n        3. 组织数据并返回包含以下三列的 DataFrame：\n            - `sequence`: 肽序列\n            - `MIC`: 计算得到的平均 MIC 值\n            - `type`: 样本类型（1 代表正样本）\n\n        Args:\n            data (pd.DataFrame): 经过筛选的正样本数据。\n\n        Returns:\n            pd.DataFrame: 包含肽序列、平均 MIC 值和样本类型的 DataFrame。\n        \"\"\"\n\n        data = pd.read_csv(positive_file_path, encoding=\"utf8\")\n        data = self.filter_data_with_str(\"bacterium\", \"aureus\", data)\n        data = self.positive_data_filter(data)\n        \n        data_all = [[], [], []]\n        for i in data[\"sequence\"].unique():\n            data_all[0].append(i)\n            log_num = 0\n            count = 0\n            for i in data[data[\"sequence\"] == i][\"value\"]:\n                log_num += math.pow(10, i)  # 计算 10^MIC 值的总和\n                count += 1\n            avg_MIC = float(log_num / count)\n            data_all[1].append(avg_MIC)  # 计算几何平均 MIC 值\n            data_all[2].append(1)  # 该肽序列属于正样本，标记为 1\n\n        data_all = list(map(list, zip(*data_all)))\n        data = pd.DataFrame(data=data_all, columns=[\"sequence\", \"MIC\", \"type\"])\n        return data\n\n    def generate_negative_data(self, negative_file_path):\n        \"\"\"读取并处理负样本数据。\n\n        过滤规则：\n        1. 移除包含 B、X、Z、O、U 的序列\n        2. 仅保留长度小于 50 的序列\n\n        Args:\n            negative_file_path (str): 负样本数据文件路径。\n\n        Returns:\n            pd.DataFrame: 处理后的负样本数据。\n        \"\"\"\n        data_negative = pd.read_csv(negative_file_path, encoding=\"utf8\")\n        data_negative = data_negative[~data_negative[\"Sequence\"].str.contains(\"B|X|Z|O|U\")]\n        data_negative = data_negative[data_negative[\"Sequence\"].apply(lambda x: 5 < len(x) <= 50)]\n        data_negative.reset_index(drop=True, inplace=True)\n        data = pd.DataFrame(columns=[\"sequence\", \"MIC\", \"type\"])\n        for i in range(data_negative.shape[0]):\n            # data = data.append({\"sequence\": data_negative[\"Sequence\"][i], \"MIC\": 8196, \"type\": 0}, ignore_index=True)\n            data = pd.concat([data, pd.DataFrame([{\"sequence\": data_negative[\"Sequence\"][i], \"MIC\": 8196, \"type\": 0}])], ignore_index=True)\n        data = data.drop_duplicates()\n        return data\n    \n    def data2csv(self, data, file_name):\n        \"\"\"将 DataFrame 保存为 CSV 文件。\n\n        Args:\n            data (pd.DataFrame): 需要保存的数据。\n            file_name (str): 输出 CSV 文件路径。\n        \"\"\"\n        data.to_csv(file_name, encoding=\"utf8\", index=False)\n    \n    def concat_datasets(self, positive_file, negative_file):\n        \"\"\"合并正样本和负样本数据，并随机打乱顺序。\n\n        Args:\n            positive_file (pd.DataFrame): 处理后的正样本数据。\n            negative_file (pd.DataFrame): 处理后的负样本数据。\n\n        Returns:\n            pd.DataFrame: 合并后的数据集。\n        \"\"\"\n        data_concat = pd.concat([positive_file, negative_file], ignore_index=True, axis=0)  # 默认纵向合并0 横向合并1\n        data_concat = data_concat.sample(frac=1, random_state=None)\n        data_concat.reset_index(drop=True, inplace=True)\n        return data_concat\n\n    def split_sample(self, sample, tag, train_ratio=0.8):\n        \"\"\"将数据集拆分为训练集和测试集，并保存为 CSV 文件。\n\n        训练集占 80%，测试集占 20%。\n\n        Args:\n            sample (pd.DataFrame): 需要拆分的数据集。\n        \"\"\"\n        num = len(sample)\n        train_sample = sample[:int(train_ratio * num)]\n        test_sample = sample[int(train_ratio * num):]\n        self.data2csv(train_sample, os.path.join(self.generate_example_path, f\"{tag}_train_sample.csv\"))\n        self.data2csv(test_sample, os.path.join(self.generate_example_path, f\"{tag}_test_sample.csv\"))","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef ensure_folder_exists(folder_path):\n    \"\"\"\n    确保文件夹存在。如果文件夹不存在，则创建它。\n\n    Args:\n        folder_path (str): 要创建的文件夹路径。\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)  # 创建文件夹\n        print(f\"已创建新文件夹: {folder_path}\")\n    else:\n        print(f\"文件夹已存在: {folder_path}\")\n\nensure_folder_exists(\"data\")","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"已创建新文件夹: data\n","output_type":"stream"}]},{"cell_type":"code","source":"gramapa_path = \"downloads/199116/grampa.csv\"\nnegative_file_path = \"downloads/199116/origin_negative.csv\"\ngenerate_example_path = \"data\"\nmode = \"all\"\ngenerate_sample = GenerateSample(gramapa_path, negative_file_path, generate_example_path, mode)\ngenerate_sample()\n\n# 二、模型训练\n\n## 2.1 XGBoost 分类器","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"generating xgboost sample.....\n开始计算，共有 7769 条序列，使用 4 个进程\n已处理 1000/7769 条序列 | 进程 ID: 241 | 进程统计: {241: 269, 243: 234, 242: 247, 244: 250}\n已处理 2000/7769 条序列 | 进程 ID: 244 | 进程统计: {241: 515, 243: 487, 242: 491, 244: 507}\n已处理 3000/7769 条序列 | 进程 ID: 242 | 进程统计: {241: 771, 243: 725, 242: 746, 244: 758}\n已处理 4000/7769 条序列 | 进程 ID: 242 | 进程统计: {241: 1034, 243: 973, 242: 1000, 244: 993}\n已处理 5000/7769 条序列 | 进程 ID: 241 | 进程统计: {241: 1283, 243: 1218, 242: 1256, 244: 1243}\n已处理 6000/7769 条序列 | 进程 ID: 244 | 进程统计: {241: 1517, 243: 1478, 242: 1498, 244: 1507}\n已处理 7000/7769 条序列 | 进程 ID: 243 | 进程统计: {241: 1761, 243: 1743, 242: 1745, 244: 1751}\n所有序列处理完成！共 7769 条\n进程工作统计: {241: 1944, 243: 1923, 242: 1947, 244: 1955}\ngenerate xgboost feature data cost time: 0.05625973181737438\ngenerating lstm sample.....\nlstm sample is ok\n__call__ 执行时间: 440.5697 秒\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.1 XGBoost 分类器","metadata":{}},{"cell_type":"code","source":"# Pip 21.3+ is required\n! pip --version\n! pip install --upgrade pip","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"pip 23.0.1 from /usr/local/lib/python3.10/site-packages/pip (python 3.10)\nLooking in indexes: https://mirrors.aliyun.com/pypi/simple\nRequirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\nCollecting pip\n  Downloading https://mirrors.aliyun.com/pypi/packages/c9/bc/b7db44f5f39f9d0494071bddae6880eb645970366d0a200022a1a93d57f5/pip-25.0.1-py3-none-any.whl (1.8 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.0.1\n    Uninstalling pip-23.0.1:\n      Successfully uninstalled pip-23.0.1\nSuccessfully installed pip-25.0.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m","output_type":"stream"}]},{"cell_type":"code","source":"! pip install xgboost==3.0.0","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Looking in indexes: https://mirrors.aliyun.com/pypi/simple\nCollecting xgboost==3.0.0\n  Downloading https://mirrors.aliyun.com/pypi/packages/63/f1/653afe1a1b7e1d03f26fd4bd30f3eebcfac2d8982e1a85b6be3355dcae25/xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m253.9/253.9 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:06\u001B[0m\n\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from xgboost==3.0.0) (1.26.3)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/site-packages (from xgboost==3.0.0) (2.20.5)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from xgboost==3.0.0) (1.12.0)\n\u001B[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n    torch (>=1.9.*)\n           ~~~~~~^\u001B[0m\u001B[33m\n\u001B[0mInstalling collected packages: xgboost\nSuccessfully installed xgboost-3.0.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n\u001B[0m","output_type":"stream"}]},{"cell_type":"code","source":"from utils import get_train_xgb_classifier_data\nimport xgboost\nfrom xgboost.callback import EarlyStopping\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom collections import Counter\nimport joblib\nimport numpy as np","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"@timer\ndef train_xgboost_classifier(train_data_path, test_data_path):\n    # 加载数据\n    x_train, x_test, y_train, y_test = get_train_xgb_classifier_data(train_data_path, test_data_path)\n    \n    # 查看类别分布\n    train_counter = Counter(y_train)\n    test_counter = Counter(y_test)\n    \n    # 计算类别比例\n    train_total = sum(train_counter.values())\n    test_total = sum(test_counter.values())\n\n    train_ratio = {k: v / train_total for k, v in train_counter.items()}\n    test_ratio = {k: v / test_total for k, v in test_counter.items()}\n\n    # 计算 scale_pos_weight\n    scale_pos_weight = train_counter[0] / max(1, train_counter[1])\n\n    # 打印信息\n    print(\"==== 类别分布 ====\")\n    print(f\"训练集类别分布: {train_counter}\")\n    print(f\"测试集类别分布: {test_counter}\\n\")\n\n    print(\"==== 类别比例 ====\")\n    print(f\"训练集类别比例: {train_ratio}\")\n    print(f\"测试集类别比例: {test_ratio}\\n\")\n\n    print(\"==== 类别不平衡分析 ====\")\n    if train_ratio[1] < 0.2:\n        print(f\"训练集类别 1 仅占 {train_ratio[1]:.2%}，数据存在类别不平衡问题！\")\n        print(f\"建议设置 XGBoost 的 `scale_pos_weight={scale_pos_weight:.2f}`\\n\")\n    else:\n        print(\"训练集类别相对平衡。\\n\")\n\n    print(\"==== 数据拆分合理性检查 ====\")\n    if abs(train_ratio[1] - test_ratio[1]) < 0.02:  # 差异小于 2%\n        print(\"训练集和测试集类别比例接近，拆分合理。\\n\")\n    else:\n        print(f\"测试集类别 1 占比 {test_ratio[1]:.2%}，与训练集 {train_ratio[1]:.2%} 差异较大，可能需要重新拆分！\\n\")\n    \n    \n    # 参数网格\n    param_grid = {\n        'learning_rate': [0.1, 0.3, 0.5, 0.7],\n        'max_depth': [4, 6, 8, 10],\n        'n_estimators': [300, 600, 900, 1200]\n    }\n    \n    xgb_model = xgboost.XGBClassifier(\n        use_label_encoder=False,\n        objective=\"binary:logistic\",\n        tree_method=\"hist\",\n        device=\"cuda\",\n        scale_pos_weight=scale_pos_weight,\n        eval_metric='auc',\n        verbosity=0\n    )\n    \n    grid_search = GridSearchCV(\n        estimator=xgb_model,\n        param_grid=param_grid,\n        scoring='f1',\n        cv=3,\n        verbose=2,\n        n_jobs=-1\n    )\n    \n    # 不传递额外参数给fit方法\n    grid_search.fit(x_train, y_train)\n    \n    # 获取最佳参数\n    best_parameters = grid_search.best_params_\n    print(\"最佳参数:\", best_parameters)\n    print(\"最佳F1分数:\", grid_search.best_score_)\n    \n    \n    # 使用早停 - 注意eval_set是合法的fit参数\n    # 创建验证集（用于早停）\n    x_train_split, x_valid, y_train_split, y_valid = train_test_split(\n        x_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n    )\n    early_stop = EarlyStopping(rounds=50,\n                               metric_name='auc',\n                               data_name='validation_0')\n    \n    # 使用最佳参数训练最终模型\n    final_model = xgboost.XGBClassifier(\n        max_depth=best_parameters[\"max_depth\"],\n        n_estimators=best_parameters[\"n_estimators\"],\n        learning_rate=best_parameters[\"learning_rate\"],\n        use_label_encoder=False,\n        objective=\"binary:logistic\",\n        tree_method=\"hist\",\n        device=\"cuda\",\n        scale_pos_weight=scale_pos_weight,\n        eval_metric='auc',\n        callbacks=[early_stop],\n        verbosity=0\n    )\n    \n    # 在 fit 方法中使用回调\n    final_model.fit(\n        x_train_split, y_train_split,\n        eval_set=[(x_valid, y_valid)],\n        verbose=True\n    )\n    \n    # 保存模型\n    joblib.dump(final_model, \"xgb_classifier_model.pkl\")\n    \n    # 评估最终模型\n    y_pred = final_model.predict(x_test)\n    \n    print(\"混淆矩阵:\")\n    print(confusion_matrix(y_test, y_pred))\n    print(\"\\n分类报告:\")\n    print(classification_report(y_test, y_pred))\n    print(f\"准确率: {accuracy_score(y_test, y_pred):.4f}\")\n    print(f\"F1分数: {f1_score(y_test, y_pred):.4f}\")","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_xgboost_classifier(\n    \"data/xgb_train_sample.csv\",\n    \"data/xgb_test_sample.csv\"\n)","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"==== 类别分布 ====\n训练集类别分布: Counter({0: 4687, 1: 1528})\n测试集类别分布: Counter({0: 1170, 1: 384})\n\n==== 类别比例 ====\n训练集类别比例: {0: 0.7541432019308125, 1: 0.24585679806918745}\n测试集类别比例: {0: 0.752895752895753, 1: 0.2471042471042471}\n\n==== 类别不平衡分析 ====\n训练集类别相对平衡。\n\n==== 数据拆分合理性检查 ====\n训练集和测试集类别比例接近，拆分合理。\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n最佳参数: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 900}\n最佳F1分数: 0.9393589585990361\n[0]\tvalidation_0-auc:0.93101\n[1]\tvalidation_0-auc:0.93594\n[2]\tvalidation_0-auc:0.95650\n[3]\tvalidation_0-auc:0.96077\n[4]\tvalidation_0-auc:0.96073\n[5]\tvalidation_0-auc:0.96317\n[6]\tvalidation_0-auc:0.96437\n[7]\tvalidation_0-auc:0.96894\n[8]\tvalidation_0-auc:0.96914\n[9]\tvalidation_0-auc:0.97505\n[10]\tvalidation_0-auc:0.97561\n[11]\tvalidation_0-auc:0.97583\n[12]\tvalidation_0-auc:0.97722\n[13]\tvalidation_0-auc:0.97815\n[14]\tvalidation_0-auc:0.97900\n[15]\tvalidation_0-auc:0.97992\n[16]\tvalidation_0-auc:0.97993\n[17]\tvalidation_0-auc:0.98095\n[18]\tvalidation_0-auc:0.98163\n[19]\tvalidation_0-auc:0.98183\n[20]\tvalidation_0-auc:0.98258\n[21]\tvalidation_0-auc:0.98344\n[22]\tvalidation_0-auc:0.98375\n[23]\tvalidation_0-auc:0.98391\n[24]\tvalidation_0-auc:0.98446\n[25]\tvalidation_0-auc:0.98463\n[26]\tvalidation_0-auc:0.98498\n[27]\tvalidation_0-auc:0.98515\n[28]\tvalidation_0-auc:0.98571\n[29]\tvalidation_0-auc:0.98597\n[30]\tvalidation_0-auc:0.98610\n[31]\tvalidation_0-auc:0.98628\n[32]\tvalidation_0-auc:0.98634\n[33]\tvalidation_0-auc:0.98663\n[34]\tvalidation_0-auc:0.98665\n[35]\tvalidation_0-auc:0.98733\n[36]\tvalidation_0-auc:0.98769\n[37]\tvalidation_0-auc:0.98771\n[38]\tvalidation_0-auc:0.98787\n[39]\tvalidation_0-auc:0.98802\n[40]\tvalidation_0-auc:0.98815\n[41]\tvalidation_0-auc:0.98824\n[42]\tvalidation_0-auc:0.98841\n[43]\tvalidation_0-auc:0.98850\n[44]\tvalidation_0-auc:0.98845\n[45]\tvalidation_0-auc:0.98867\n[46]\tvalidation_0-auc:0.98875\n[47]\tvalidation_0-auc:0.98886\n[48]\tvalidation_0-auc:0.98892\n[49]\tvalidation_0-auc:0.98909\n[50]\tvalidation_0-auc:0.98920\n[51]\tvalidation_0-auc:0.98930\n[52]\tvalidation_0-auc:0.98921\n[53]\tvalidation_0-auc:0.98940\n[54]\tvalidation_0-auc:0.98957\n[55]\tvalidation_0-auc:0.98962\n[56]\tvalidation_0-auc:0.98958\n[57]\tvalidation_0-auc:0.98955\n[58]\tvalidation_0-auc:0.98964\n[59]\tvalidation_0-auc:0.98980\n[60]\tvalidation_0-auc:0.98993\n[61]\tvalidation_0-auc:0.99003\n[62]\tvalidation_0-auc:0.99012\n[63]\tvalidation_0-auc:0.99019\n[64]\tvalidation_0-auc:0.99020\n[65]\tvalidation_0-auc:0.99043\n[66]\tvalidation_0-auc:0.99044\n[67]\tvalidation_0-auc:0.99061\n[68]\tvalidation_0-auc:0.99068\n[69]\tvalidation_0-auc:0.99078\n[70]\tvalidation_0-auc:0.99069\n[71]\tvalidation_0-auc:0.99081\n[72]\tvalidation_0-auc:0.99090\n[73]\tvalidation_0-auc:0.99091\n[74]\tvalidation_0-auc:0.99102\n[75]\tvalidation_0-auc:0.99103\n[76]\tvalidation_0-auc:0.99096\n[77]\tvalidation_0-auc:0.99106\n[78]\tvalidation_0-auc:0.99101\n[79]\tvalidation_0-auc:0.99110\n[80]\tvalidation_0-auc:0.99117\n[81]\tvalidation_0-auc:0.99109\n[82]\tvalidation_0-auc:0.99109\n[83]\tvalidation_0-auc:0.99111\n[84]\tvalidation_0-auc:0.99118\n[85]\tvalidation_0-auc:0.99101\n[86]\tvalidation_0-auc:0.99108\n[87]\tvalidation_0-auc:0.99117\n[88]\tvalidation_0-auc:0.99122\n[89]\tvalidation_0-auc:0.99128\n[90]\tvalidation_0-auc:0.99133\n[91]\tvalidation_0-auc:0.99124\n[92]\tvalidation_0-auc:0.99131\n[93]\tvalidation_0-auc:0.99135\n[94]\tvalidation_0-auc:0.99137\n[95]\tvalidation_0-auc:0.99141\n[96]\tvalidation_0-auc:0.99143\n[97]\tvalidation_0-auc:0.99148\n[98]\tvalidation_0-auc:0.99148\n[99]\tvalidation_0-auc:0.99156\n[100]\tvalidation_0-auc:0.99162\n[101]\tvalidation_0-auc:0.99165\n[102]\tvalidation_0-auc:0.99166\n[103]\tvalidation_0-auc:0.99166\n[104]\tvalidation_0-auc:0.99179\n[105]\tvalidation_0-auc:0.99183\n[106]\tvalidation_0-auc:0.99184\n[107]\tvalidation_0-auc:0.99190\n[108]\tvalidation_0-auc:0.99193\n[109]\tvalidation_0-auc:0.99191\n[110]\tvalidation_0-auc:0.99194\n[111]\tvalidation_0-auc:0.99198\n[112]\tvalidation_0-auc:0.99208\n[113]\tvalidation_0-auc:0.99209\n[114]\tvalidation_0-auc:0.99214\n[115]\tvalidation_0-auc:0.99223\n[116]\tvalidation_0-auc:0.99228\n[117]\tvalidation_0-auc:0.99226\n[118]\tvalidation_0-auc:0.99233\n[119]\tvalidation_0-auc:0.99233\n[120]\tvalidation_0-auc:0.99240\n[121]\tvalidation_0-auc:0.99239\n[122]\tvalidation_0-auc:0.99245\n[123]\tvalidation_0-auc:0.99252\n[124]\tvalidation_0-auc:0.99254\n[125]\tvalidation_0-auc:0.99254\n[126]\tvalidation_0-auc:0.99248\n[127]\tvalidation_0-auc:0.99253\n[128]\tvalidation_0-auc:0.99259\n[129]\tvalidation_0-auc:0.99253\n[130]\tvalidation_0-auc:0.99256\n[131]\tvalidation_0-auc:0.99257\n[132]\tvalidation_0-auc:0.99263\n[133]\tvalidation_0-auc:0.99268\n[134]\tvalidation_0-auc:0.99269\n[135]\tvalidation_0-auc:0.99272\n[136]\tvalidation_0-auc:0.99267\n[137]\tvalidation_0-auc:0.99270\n[138]\tvalidation_0-auc:0.99271\n[139]\tvalidation_0-auc:0.99276\n[140]\tvalidation_0-auc:0.99277\n[141]\tvalidation_0-auc:0.99271\n[142]\tvalidation_0-auc:0.99267\n[143]\tvalidation_0-auc:0.99272\n[144]\tvalidation_0-auc:0.99273\n[145]\tvalidation_0-auc:0.99280\n[146]\tvalidation_0-auc:0.99283\n[147]\tvalidation_0-auc:0.99286\n[148]\tvalidation_0-auc:0.99286\n[149]\tvalidation_0-auc:0.99292\n[150]\tvalidation_0-auc:0.99291\n[151]\tvalidation_0-auc:0.99297\n[152]\tvalidation_0-auc:0.99289\n[153]\tvalidation_0-auc:0.99290\n[154]\tvalidation_0-auc:0.99296\n[155]\tvalidation_0-auc:0.99296\n[156]\tvalidation_0-auc:0.99297\n[157]\tvalidation_0-auc:0.99296\n[158]\tvalidation_0-auc:0.99296\n[159]\tvalidation_0-auc:0.99294\n[160]\tvalidation_0-auc:0.99299\n[161]\tvalidation_0-auc:0.99299\n[162]\tvalidation_0-auc:0.99301\n[163]\tvalidation_0-auc:0.99301\n[164]\tvalidation_0-auc:0.99305\n[165]\tvalidation_0-auc:0.99306\n[166]\tvalidation_0-auc:0.99306\n[167]\tvalidation_0-auc:0.99308\n[168]\tvalidation_0-auc:0.99316\n[169]\tvalidation_0-auc:0.99317\n[170]\tvalidation_0-auc:0.99316\n[171]\tvalidation_0-auc:0.99321\n[172]\tvalidation_0-auc:0.99319\n[173]\tvalidation_0-auc:0.99318\n[174]\tvalidation_0-auc:0.99317\n[175]\tvalidation_0-auc:0.99317\n[176]\tvalidation_0-auc:0.99318\n[177]\tvalidation_0-auc:0.99317\n[178]\tvalidation_0-auc:0.99313\n[179]\tvalidation_0-auc:0.99320\n[180]\tvalidation_0-auc:0.99318\n[181]\tvalidation_0-auc:0.99314\n[182]\tvalidation_0-auc:0.99310\n[183]\tvalidation_0-auc:0.99310\n[184]\tvalidation_0-auc:0.99308\n[185]\tvalidation_0-auc:0.99308\n[186]\tvalidation_0-auc:0.99311\n[187]\tvalidation_0-auc:0.99312\n[188]\tvalidation_0-auc:0.99313\n[189]\tvalidation_0-auc:0.99305\n[190]\tvalidation_0-auc:0.99304\n[191]\tvalidation_0-auc:0.99304\n[192]\tvalidation_0-auc:0.99301\n[193]\tvalidation_0-auc:0.99300\n[194]\tvalidation_0-auc:0.99297\n[195]\tvalidation_0-auc:0.99296\n[196]\tvalidation_0-auc:0.99295\n[197]\tvalidation_0-auc:0.99292\n[198]\tvalidation_0-auc:0.99301\n[199]\tvalidation_0-auc:0.99308\n[200]\tvalidation_0-auc:0.99315\n[201]\tvalidation_0-auc:0.99315\n[202]\tvalidation_0-auc:0.99314\n[203]\tvalidation_0-auc:0.99315\n[204]\tvalidation_0-auc:0.99314\n[205]\tvalidation_0-auc:0.99313\n[206]\tvalidation_0-auc:0.99314\n[207]\tvalidation_0-auc:0.99321\n[208]\tvalidation_0-auc:0.99321\n[209]\tvalidation_0-auc:0.99321\n[210]\tvalidation_0-auc:0.99317\n[211]\tvalidation_0-auc:0.99312\n[212]\tvalidation_0-auc:0.99315\n[213]\tvalidation_0-auc:0.99315\n[214]\tvalidation_0-auc:0.99320\n[215]\tvalidation_0-auc:0.99320\n[216]\tvalidation_0-auc:0.99318\n[217]\tvalidation_0-auc:0.99317\n[218]\tvalidation_0-auc:0.99315\n[219]\tvalidation_0-auc:0.99307\n[220]\tvalidation_0-auc:0.99304\n混淆矩阵:\n[[1139   31]\n [  19  365]]\n\n分类报告:\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1170\n           1       0.92      0.95      0.94       384\n\n    accuracy                           0.97      1554\n   macro avg       0.95      0.96      0.96      1554\nweighted avg       0.97      0.97      0.97      1554\n\n准确率: 0.9678\nF1分数: 0.9359\ntrain_xgboost_classifier 执行时间: 1487.5929 秒\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.2 XGBoost 排序器","metadata":{}},{"cell_type":"code","source":"from utils import get_train_xgb_rank_featured_data\nfrom functools import partial\n\n@timer\ndef train_xgboost_rank(train_data_path, test_data_path):\n    \"\"\"训练一个 XGBoost 排序模型（Learning to Rank），用于 MIC 值预测，并基于排名计算 Top-k 准确率。\n\n    Args:\n        train_data_path (str): 训练数据集的文件路径。\n        test_data_path (str): 测试数据集的文件路径。\n\n    Returns:\n        None\n    \"\"\"\n\n    x_train, x_test, y_train, y_test, test_data = get_train_xgb_rank_featured_data(train_data_path, test_data_path)\n\n    xgb_model = xgboost.XGBRegressor(\n        max_depth=3,\n        n_estimators=200,\n        learning_rate=0.2,\n        use_label_encoder=False,\n        objective=\"rank:pairwise\",\n        tree_method=\"hist\",\n        device=\"cuda\",\n        eval_metric=\"auc\",\n        verbosity=0\n    )\n    \n    xgb_model.fit(x_train, y_train)\n    \n    # 评估最终模型\n    y_pred = xgb_model.predict(x_test)\n    \n    df = pd.DataFrame(test_data[\"sequence\"].copy(), columns=[\"sequence\"])\n    df[\"MIC\"] = y_pred\n    df.sort_values(\"MIC\", inplace=True)\n    for k in [50, 100, 500]:\n        true_sequecne_topk = set(test_data.iloc[:k, ][\"sequence\"])\n        pred_sequence_topk = set(df.iloc[:k, ][\"sequence\"])\n        \n        score = len(true_sequecne_topk & pred_sequence_topk) / k\n        print(f\"xgb_rank top{k} hit rate:\", score)\n    \n    # 保存模型\n    joblib.dump(xgb_model, \"xgb_rank_model.pkl\")","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_xgboost_rank(\n    \"data/xgb_train_sample.csv\",\n    \"data/xgb_test_sample.csv\"\n)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"xgb_rank top50 hit rate: 0.16\nxgb_rank top100 hit rate: 0.23\nxgb_rank top500 hit rate: 0.692\ntrain_xgboost_rank 执行时间: 3.5369 秒\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=300; total time=   4.5s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=600; total time=  14.2s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=600; total time=  10.5s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=900; total time=  17.1s\n[CV] END ..learning_rate=0.1, max_depth=4, n_estimators=1200; total time=  24.5s\n[CV] END ..learning_rate=0.1, max_depth=4, n_estimators=1200; total time=  19.8s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=300; total time=  10.8s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=600; total time=  15.3s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=900; total time=  23.6s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=900; total time=  19.9s\n[CV] END ..learning_rate=0.1, max_depth=6, n_estimators=1200; total time=  24.8s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=300; total time=  10.0s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=300; total time=   7.8s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=300; total time=  10.9s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=600; total time=  15.1s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=900; total time=  23.6s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=900; total time=  21.0s\n[CV] END ..learning_rate=0.1, max_depth=8, n_estimators=1200; total time=  27.4s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   9.9s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   9.7s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=600; total time=  16.6s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=900; total time=  24.2s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=900; total time=  25.1s\n[CV] END .learning_rate=0.1, max_depth=10, n_estimators=1200; total time=  29.1s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=300; total time=   6.4s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=300; total time=   4.1s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=300; total time=   7.0s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=600; total time=   9.2s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=600; total time=  10.1s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=900; total time=  12.2s\n[CV] END ..learning_rate=0.3, max_depth=4, n_estimators=1200; total time=  19.4s\n[CV] END ..learning_rate=0.3, max_depth=4, n_estimators=1200; total time=  19.8s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=300; total time=   7.4s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=600; total time=  10.7s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=900; total time=  17.3s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=900; total time=  16.7s\n[CV] END ..learning_rate=0.3, max_depth=6, n_estimators=1200; total time=  20.1s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=300; total time=   8.0s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=300; total time=   7.5s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=600; total time=  15.2s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=600; total time=  13.2s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=900; total time=  19.3s\n[CV] END ..learning_rate=0.3, max_depth=8, n_estimators=1200; total time=  24.9s\n[CV] END ..learning_rate=0.3, max_depth=8, n_estimators=1200; total time=  28.3s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=600; total time=  15.8s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=600; total time=  13.5s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=900; total time=  20.2s\n[CV] END .learning_rate=0.3, max_depth=10, n_estimators=1200; total time=  25.9s\n[CV] END .learning_rate=0.3, max_depth=10, n_estimators=1200; total time=  25.4s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=300; total time=   6.3s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=600; total time=   8.8s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=900; total time=  14.5s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=900; total time=  15.3s\n[CV] END ..learning_rate=0.5, max_depth=4, n_estimators=1200; total time=  16.9s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=300; total time=   7.6s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=300; total time=   7.3s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=300; total time=   4.7s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=600; total time=  12.8s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=900; total time=  17.6s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=900; total time=  16.9s\n[CV] END ..learning_rate=0.5, max_depth=6, n_estimators=1200; total time=  19.4s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=300; total time=   8.5s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=300; total time=   6.9s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=600; total time=  14.3s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=900; total time=  19.6s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=900; total time=  17.8s\n[CV] END ..learning_rate=0.5, max_depth=8, n_estimators=1200; total time=  21.6s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=300; total time=   9.6s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=300; total time=   5.1s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=300; total time=   9.7s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=600; total time=  10.6s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=900; total time=  20.5s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=900; total time=  17.6s\n[CV] END .learning_rate=0.5, max_depth=10, n_estimators=1200; total time=  24.7s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=300; total time=   4.5s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=300; total time=   8.1s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=300; total time=   5.9s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=600; total time=  11.2s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=600; total time=   9.1s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=900; total time=  15.4s\n[CV] END ..learning_rate=0.7, max_depth=4, n_estimators=1200; total time=  21.9s\n[CV] END ..learning_rate=0.7, max_depth=4, n_estimators=1200; total time=  17.8s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=600; total time=  11.7s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=600; total time=  14.1s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=900; total time=  15.7s\n[CV] END ..learning_rate=0.7, max_depth=6, n_estimators=1200; total time=  21.2s\n[CV] END ..learning_rate=0.7, max_depth=6, n_estimators=1200; total time=  19.5s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=600; total time=  14.0s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=600; total time=  10.8s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=900; total time=  14.7s\n[CV] END ..learning_rate=0.7, max_depth=8, n_estimators=1200; total time=  23.2s\n[CV] END ..learning_rate=0.7, max_depth=8, n_estimators=1200; total time=  21.8s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=300; total time=   6.5s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=600; total time=  11.3s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=900; total time=  20.7s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=900; total time=  19.0s\n[CV] END .learning_rate=0.7, max_depth=10, n_estimators=1200; total time=  23.5s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=300; total time=   4.5s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=300; total time=   7.7s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=600; total time=  10.3s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=900; total time=  17.8s\n[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=900; total time=  17.2s\n[CV] END ..learning_rate=0.1, max_depth=4, n_estimators=1200; total time=  22.7s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=300; total time=   8.7s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=300; total time=   8.7s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=600; total time=  16.2s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=600; total time=  16.5s\n[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=900; total time=  21.1s\n[CV] END ..learning_rate=0.1, max_depth=6, n_estimators=1200; total time=  26.5s\n[CV] END ..learning_rate=0.1, max_depth=6, n_estimators=1200; total time=  25.4s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=600; total time=  16.8s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=600; total time=  15.8s\n[CV] END ...learning_rate=0.1, max_depth=8, n_estimators=900; total time=  19.3s\n[CV] END ..learning_rate=0.1, max_depth=8, n_estimators=1200; total time=  29.5s\n[CV] END ..learning_rate=0.1, max_depth=8, n_estimators=1200; total time=  26.2s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=300; total time=   9.7s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=600; total time=  16.0s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=600; total time=  15.9s\n[CV] END ..learning_rate=0.1, max_depth=10, n_estimators=900; total time=  23.7s\n[CV] END .learning_rate=0.1, max_depth=10, n_estimators=1200; total time=  30.7s\n[CV] END .learning_rate=0.1, max_depth=10, n_estimators=1200; total time=  28.1s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=600; total time=   7.2s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=900; total time=  16.8s\n[CV] END ...learning_rate=0.3, max_depth=4, n_estimators=900; total time=  14.8s\n[CV] END ..learning_rate=0.3, max_depth=4, n_estimators=1200; total time=  19.5s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=300; total time=   5.9s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=300; total time=   7.5s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=600; total time=  12.2s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=600; total time=  13.6s\n[CV] END ...learning_rate=0.3, max_depth=6, n_estimators=900; total time=  14.8s\n[CV] END ..learning_rate=0.3, max_depth=6, n_estimators=1200; total time=  22.3s\n[CV] END ..learning_rate=0.3, max_depth=6, n_estimators=1200; total time=  20.2s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=300; total time=   5.1s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=600; total time=  14.5s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=900; total time=  19.5s\n[CV] END ...learning_rate=0.3, max_depth=8, n_estimators=900; total time=  20.8s\n[CV] END ..learning_rate=0.3, max_depth=8, n_estimators=1200; total time=  25.8s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=300; total time=   7.2s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=300; total time=   9.7s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=300; total time=   7.4s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=600; total time=  14.8s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=900; total time=  21.9s\n[CV] END ..learning_rate=0.3, max_depth=10, n_estimators=900; total time=  21.5s\n[CV] END .learning_rate=0.3, max_depth=10, n_estimators=1200; total time=  25.3s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=300; total time=   5.6s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=300; total time=   6.1s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=600; total time=   9.3s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=600; total time=  12.3s\n[CV] END ...learning_rate=0.5, max_depth=4, n_estimators=900; total time=  14.6s\n[CV] END ..learning_rate=0.5, max_depth=4, n_estimators=1200; total time=  20.7s\n[CV] END ..learning_rate=0.5, max_depth=4, n_estimators=1200; total time=  18.1s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=600; total time=  12.3s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=600; total time=   9.2s\n[CV] END ...learning_rate=0.5, max_depth=6, n_estimators=900; total time=  14.8s\n[CV] END ..learning_rate=0.5, max_depth=6, n_estimators=1200; total time=  22.4s\n[CV] END ..learning_rate=0.5, max_depth=6, n_estimators=1200; total time=  22.0s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=300; total time=   6.0s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=600; total time=  14.0s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=600; total time=  12.6s\n[CV] END ...learning_rate=0.5, max_depth=8, n_estimators=900; total time=  15.3s\n[CV] END ..learning_rate=0.5, max_depth=8, n_estimators=1200; total time=  25.5s\n[CV] END ..learning_rate=0.5, max_depth=8, n_estimators=1200; total time=  21.3s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=600; total time=  15.1s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=600; total time=  16.0s\n[CV] END ..learning_rate=0.5, max_depth=10, n_estimators=900; total time=  18.3s\n[CV] END .learning_rate=0.5, max_depth=10, n_estimators=1200; total time=  27.4s\n[CV] END .learning_rate=0.5, max_depth=10, n_estimators=1200; total time=  23.8s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=600; total time=  11.6s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=900; total time=  15.4s\n[CV] END ...learning_rate=0.7, max_depth=4, n_estimators=900; total time=  13.4s\n[CV] END ..learning_rate=0.7, max_depth=4, n_estimators=1200; total time=  19.8s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=300; total time=   8.0s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=300; total time=   4.8s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=300; total time=   7.8s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=600; total time=   8.1s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=900; total time=  17.5s\n[CV] END ...learning_rate=0.7, max_depth=6, n_estimators=900; total time=  16.6s\n[CV] END ..learning_rate=0.7, max_depth=6, n_estimators=1200; total time=  17.9s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=300; total time=   9.0s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=300; total time=   5.3s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=300; total time=   7.2s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=600; total time=  10.0s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=900; total time=  17.7s\n[CV] END ...learning_rate=0.7, max_depth=8, n_estimators=900; total time=  17.4s\n[CV] END ..learning_rate=0.7, max_depth=8, n_estimators=1200; total time=  21.6s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=300; total time=   9.0s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=300; total time=   4.3s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=600; total time=  15.9s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=600; total time=  12.7s\n[CV] END ..learning_rate=0.7, max_depth=10, n_estimators=900; total time=  17.4s\n[CV] END .learning_rate=0.7, max_depth=10, n_estimators=1200; total time=  23.7s\n[CV] END .learning_rate=0.7, max_depth=10, n_estimators=1200; total time=  15.7s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.3 LSTM 回归器","metadata":{}},{"cell_type":"code","source":"from torch import nn\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n\nclass LstmNet(nn.Module):\n    def __init__(self, embedding_dim, hidden_num, num_layer, bidirectional, dropout, Letter_dict):\n        \"\"\"LSTM 网络模型，用于处理序列数据。\n\n        Args:\n            embedding_dim (int): 词嵌入的维度。\n            hidden_num (int): LSTM 隐藏层单元数。\n            num_layer (int): LSTM 层数。\n            bidirectional (bool): 是否使用双向 LSTM。\n            dropout (float): Dropout 比例。\n            Letter_dict (dict): 字符到索引的映射字典。\n        \"\"\"\n        super(LstmNet, self).__init__()\n        # 定义 LSTM 层\n        self.lstm = torch.nn.LSTM(\n            embedding_dim,                  # 词嵌入维度\n            hidden_num,                     # 隐藏层单元数\n            num_layer,                      # LSTM 层数\n            bidirectional=bidirectional,    # 是否为双向 LSTM\n            batch_first=True,               # 设定 batch 维度优先\n            dropout=dropout)                # Dropout 率\n        # 定义线性层（全连接层）用于输出\n        self.linear = nn.Sequential(\n            nn.Linear(hidden_num * (2 if bidirectional == True else 1), 64),    # 连接隐藏层输出到 64 维\n            nn.ReLU(inplace=True),                                              # ReLU 激活函数\n            nn.Linear(64, 1))                                                   # 最终输出层\n        # 定义词嵌入层\n        self.embedding = torch.nn.Embedding(\n            num_embeddings=len(Letter_dict) + 1,    # 词汇表大小\n            embedding_dim=embedding_dim,            # 词嵌入维度\n            padding_idx=0)                          # 设置填充索引\n\n    def forward(self, x, length):\n        \"\"\"前向传播逻辑\n\n        Args:\n            x (Tensor): 输入的序列数据 (batch_size, seq_len)。\n            length (Tensor): 每个序列的真实长度 (batch_size)。\n\n        Returns:\n            Tensor: 模型输出，形状为 (batch_size, 1)。\n        \"\"\"\n        # 通过嵌入层，将索引转换为词向量\n        x = self.embedding(x.long())\n        # 使用 pack_padded_sequence 压缩填充序列，提高 LSTM 计算效率\n        x = pack_padded_sequence(input=x, lengths=length, batch_first=True, enforce_sorted=False)\n        # 通过 LSTM 层\n        output, (h_s, h_c) = self.lstm(x)\n        # 将压缩后的序列解压回原始序列形式\n        output = pad_packed_sequence(output, batch_first=True)[0]\n        # 计算所有时间步的均值，并传入全连接层进行预测\n        out = self.linear(output.mean(dim=1))\n        return out","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from utils import df2list, get_dict, get_reverse_dict, evaluate_customized\nfrom dataset import PeptideDataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nclass LstmTrain():\n    def __init__(self, train_file, test_file):\n        self.net = LstmNet(embedding_dim, hidden_num, num_layer, bidirectional, dropout, get_dict())\n        self.net.to(device)\n        # 读取训练数据，数据共有3列，分别是[sequence, MIC, type]\n        train_data = pd.read_csv(train_file, encoding=\"utf-8\").reset_index(drop=True)\n        train_data = df2list(train_data, \"sequence\", \"MIC\", \"type\", ngram_num, log_num=10)  # 将训练数据格式化\n        # 构建数据集，一个数据单元由4部分构成，第一是序列的索引表示，定长50，不足补零；第二是log10MIC；第三是type值，0or1；第四是序列长度\n        train_dataset = PeptideDataset(train_data)\n        self.train_dataset = train_dataset\n        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n\n        # 数据加载，决定输入网络的数据形式，每次输入batch_size个数据\n        # 一个batch_size的数据内容为：[tensor([[seq], [seq], ...]),\n        #                           tensor([[log10MIC], [log10MIC], ...]),\n        #                           tensor([type, type, ...]),\n        #                           tensor([len, len, ...])]\n        # 一个batch_size的数据维度为：[torch.Size([batch_size, 50]),\n        #                           torch.Size([batch_size, 1]),\n        #                           torch.Size([batch_size]),\n        #                           torch.Size([batch_size])]\n\n        test_data = pd.read_csv(test_file, encoding=\"utf-8\").reset_index(drop=True)\n        test_data = df2list(test_data, \"sequence\", \"MIC\", \"type\", ngram_num, log_num=10)\n        test_dataset = PeptideDataset(test_data)\n        self.test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n        self.opt = torch.optim.AdamW(self.net.parameters(), lr=learning_rate)  # 优化器\n        self.loss_fn = nn.MSELoss()  # 损失函数\n\n    def __call__(self):\n        # 初始化最佳模型指标\n        loss_best_all = 10000  # 记录所有数据的最佳 MSE\n        loss_best_pos = 10000  # 记录阳性数据的最佳 MSE\n        r2_best_all = -5  # 记录所有数据的最佳 R²\n        r2_best_pos = -5  # 记录阳性数据的最佳 R²\n        top20mse_best = 99  # 记录 Top 20 MSE 最优值\n        top60mse_best = 99  # 记录 Top 60 MSE 最优值\n\n        # 训练与测试结果存储\n        test_loss_all = []\n        test_loss_pos = []\n        test_r2_all = []\n        test_r2_pos = []\n        top_20_mse = []\n        top_60_mse = []\n\n        best_model_path_all = \"all_lstm_regress_model.pth\"  # 记录所有数据的最佳模型路径\n        best_model_path_pos = \"pos_lstm_regress_model.pth\"  # 记录阳性数据的最佳模型路径\n\n        for epoch in range(epoch_num):\n            # 每 de_epo 轮，降低学习率\n            if epoch > 0 and epoch % de_epo == 0:\n                for param_group in self.opt.param_groups:\n                    param_group['lr'] *= de_r\n            \n            loss_all = 0   # 记录当前轮次的总损失\n            count = 0  # 记录 batch 数量\n            self.net.train()\n            \n            # 训练循环\n            for idx, batch in enumerate(tqdm(self.train_dataloader)):\n                text = batch[0].to(device)  # 序列输入 torch.Size([batch_size, 50])\n                label = batch[1].to(device)  # 真实 MIC 标签 torch.Size([batch_size, 1])\n                length = batch[3]  # 序列长度 torch.Size([batch_size])\n                \n                out = self.net(text, length)  # 模型向前传播\n                loss = self.loss_fn(out, label)  # 计算损失\n\n                self.opt.zero_grad()\n                loss.backward(retain_graph=True)  # 反向传播\n                self.opt.step()  # 更新参数\n\n                loss_all += loss\n                count += 1\n\n                if count % 100 == 0:\n                    print(\"\\r Epoch:%d,Loss:%f\" % (epoch, loss_all / count))\n\n            # 进入评估模式\n            self.net.eval()\n            predict, label_o, sequence = [], [], []\n            \n            # 测试循环\n            for idx, batch in enumerate(self.test_dataloader):\n                text = torch.LongTensor(batch[0])\n                label = batch[1]\n                length = batch[3]\n                text = text.to(device)\n                label = label.to(device)\n                out = self.net(text, length)\n\n                predict.extend(out.cpu().detach().numpy().reshape(1, -1)[0])\n                label_o.extend(label.cpu().detach().numpy())\n                for i in text.cpu().numpy():\n                    temp = []\n                    for j in i[i > 0]:\n                        temp.append(get_reverse_dict()[j])\n                    sequence.append(\"\".join(temp))\n            df = pd.DataFrame({\"sequence\": sequence, \"label\": label_o, \"predict\": predict})\n\n            # 仅计算阳性数据\n            pred, label = zip(*[(predict[i], label_o[i]) for i in range(len(label_o)) if label_o[i] <= np.log10(8196) - 0.01])\n            \n            # 计算 MSE 和 R² 评分\n            mse_result_all = mean_squared_error(label_o, predict)\n            mse_result_pos = mean_squared_error(label, pred)\n            r2_result_all = r2_score(label_o, predict)\n            r2_result_pos = r2_score(label, pred)\n\n            # 记录测试结果\n            test_loss_all.append(mse_result_all)\n            test_loss_pos.append(mse_result_pos)\n            test_r2_all.append(r2_result_all)\n            test_r2_pos.append(r2_result_pos)\n            \n            # 计算 Top 20 和 Top 60 MSE\n            top20_mse, top60_mse, _, _ = evaluate_customized(self.net, label, pred)\n            top_20_mse.append(top20_mse)\n            top_60_mse.append(top60_mse)\n\n            # 保存最佳模型\n            if mse_result_all <= loss_best_all:\n                loss_best_all = mse_result_all\n                torch.save(self.net.state_dict(), best_model_path_all)\n            if mse_result_pos <= loss_best_pos:\n                loss_best_pos = mse_result_pos\n                torch.save(self.net.state_dict(), best_model_path_pos)\n            if r2_result_all >= r2_best_all:\n                r2_best_all = r2_result_all\n            if r2_result_pos >= r2_best_pos:\n                r2_best_pos = r2_result_pos\n            if top20_mse <= top20mse_best:\n                top20mse_best = top20_mse\n            if top60_mse <= top60mse_best:\n                top60mse_best = top60_mse\n            \n            # 打印当前 epoch 结果\n            print(\"Top 20 MSE: \", top20_mse)\n            print(\"Top 60 MSE: \", top60_mse)\n            print(\"MSE_loss_all = %f\" % (mse_result_all))\n            print(\"R2_score_all = %f\" % (r2_result_all))\n            print(\"MSE_loss_pos = %f\" % (mse_result_pos))\n            print(\"R2_score_pos = %f\" % (r2_result_pos))\n            print(\n                \"\\r Epoch: %d Best MSE Error all %f ; Best MSE Error pos: %f ; Best R2 Error all: %f ; Best R2 Error pos: %f ; Best Top 20: %f ;Best Top 60: %f\" \\\n                % (epoch, loss_best_all, loss_best_pos, r2_best_all, r2_best_pos, top20mse_best, top60mse_best))","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 50\nhidden_num = 128\nnum_layer = 2\nbidirectional = False\ndropout = 0.7\nngram_num = 1\nlog_num = 10\nbatch_size = 16\nlearning_rate = 2 * 1e-3\nepoch_num = 100\nde_epo = 16\nde_r = 0.5\nseed = 42\nmode = \"lstm\"\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"模型训练将在{device}进行\")\nlstmtrain = LstmTrain(\n    \"data/lstm_train_sample.csv\",\n    \"data/lstm_test_sample.csv\"\n)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"模型训练将在cuda:0进行\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\n\n# 备份原始 sys.stdout\noriginal_stdout = sys.stdout\n\ntry:\n    sys.stdout = open('train_log.txt', 'w')  # 重定向输出到文件\n    lstmtrain()  # 运行训练\nfinally:\n    sys.stdout.close()  # 关闭文件\n    sys.stdout = original_stdout  # 恢复默认输出","metadata":{"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 389/389 [00:05<00:00, 74.68it/s] \n100%|██████████| 389/389 [00:04<00:00, 96.16it/s] \n100%|██████████| 389/389 [00:02<00:00, 138.90it/s]\n100%|██████████| 389/389 [00:04<00:00, 96.45it/s] \n100%|██████████| 389/389 [00:04<00:00, 96.61it/s] \n100%|██████████| 389/389 [00:04<00:00, 96.45it/s] \n100%|██████████| 389/389 [00:02<00:00, 146.56it/s]\n100%|██████████| 389/389 [00:03<00:00, 97.84it/s] \n100%|██████████| 389/389 [00:04<00:00, 96.03it/s]\n100%|██████████| 389/389 [00:04<00:00, 95.31it/s] \n100%|██████████| 389/389 [00:02<00:00, 149.12it/s]\n100%|██████████| 389/389 [00:03<00:00, 98.64it/s] \n100%|██████████| 389/389 [00:03<00:00, 97.45it/s] \n100%|██████████| 389/389 [00:04<00:00, 95.37it/s]\n100%|██████████| 389/389 [00:02<00:00, 145.93it/s]\n100%|██████████| 389/389 [00:04<00:00, 94.39it/s]\n100%|██████████| 389/389 [00:04<00:00, 95.93it/s]\n100%|██████████| 389/389 [00:03<00:00, 98.72it/s] \n100%|██████████| 389/389 [00:02<00:00, 140.13it/s]\n 68%|██████▊   | 265/389 [00:02<00:01, 94.48it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"# 三、活性肽预测\n\n## 3.1 生成用于预测的数据","metadata":{}},{"cell_type":"code","source":"# 亲水性为1 疏水性为0\nhydrophily = [\"S\", \"T\", \"C\", \"Y\", \"N\", \"Q\", \"D\", \"E\", \"K\", \"R\", \"H\"]  # 亲水性\nhydrophobe = [\"G\", \"A\", \"V\", \"L\", \"I\", \"M\", \"F\", \"W\", \"P\"]  # 疏水性\n# 氨基酸的电荷映射（e_charge）：正电荷为1，负电荷为-1，中性为0\ne_charge = {\"S\": 0, \"T\": 0, \"C\": 0, \"Y\": 0, \"N\": 0, \"Q\": 0, \"D\": -1, \"E\": -1, \"K\": 1, \"R\": 1 \\\n    , \"H\": 1, \"G\": 0, \"A\": 0, \"V\": 0, \"L\": 0, \"I\": 0, \"M\": 0, \"F\": 0, \"W\": 0, \"P\": 0}\n# 规则列表，每个字符串包含7位，代表7肽的亲水性/疏水性模式（0为疏水，1为亲水）\nrules_list = [\"0000011\", \"0000111\",\n              \"0001111\", \"0011111\",\n              \"1100000\", \"1110000\",\n              \"1111000\", \"1111100\",\n              \"1100100\", \"0010011\",\n              \"1001001\", \"1001000\",\n              \"1000100\", \"0100100\",\n              \"0010010\", \"0010001\",\n              \"0001001\"]\n# 存储符合每种规则的7肽序列的列表\nresult_list = [[] for x in rules_list]\n\n@timer\ndef get_sequence(sequence_now, charge_now, rule, rule_count):\n    \"\"\"递归生成符合规则的7肽序列，并确保至少有一个正电荷。\n\n    :param sequence_now: 当前已生成的肽序列\n    :param charge_now: 当前序列的电荷总和\n    :param rule: 7位的规则字符串，指示亲水性/疏水性模式\n    :param rule_count: 当前递归到的氨基酸位置（0-6）\n    \"\"\"\n    if rule_count == 7:  # 递归终止条件：已构建7个氨基酸\n        if charge_now > 0:  # 只有带正电荷的序列才被保留\n            result_list[rules_list.index(rule)].append(sequence_now)\n\n    else:\n        type = int(rule[rule_count])  # 读取当前位规则，0为疏水，1为亲水\n        if type == 1:   # 选择亲水性氨基酸\n            for pep in hydrophily:\n                get_sequence(sequence_now + pep, charge_now + e_charge[pep], rule, rule_count + 1)\n        else:  # 选择疏水性氨基酸\n            for pep in hydrophobe:\n                get_sequence(sequence_now + pep, charge_now + e_charge[pep], rule, rule_count + 1)\n\n# 遍历所有规则，生成符合规则的7肽序列\nfor rule in rules_list:\n    get_sequence(\"\", 0, rule, 0)\n    break\n    \n# 为7肽序列提取特征\nfile_count = 0\nfor peptides in result_list:\n    cal_pep_des_multiprocess.cal_pep_parallel(peptides=peptides, sequence=pd.Series(peptides, name=\"sequence\"),\n                                              results=pd.Series(dtype=object), types=pd.Series(dtype=object),\n                                              output_path=\"7_peptide_rule_%d.txt\" % (file_count))\n    print(f\"已生成 7_peptide_rule_{file_count}.txt\")\n    file_count += 1\n    break","metadata":{"trusted":true},"outputs":[]},{"cell_type":"code","source":"from dataset import PredictDataset\n\nclass Predict():\n    def __init__(self,xgb_classifier_path, xgb_rank_path, rank_top_k, lstm_param_path, predict_data_file, result_save_path):\n        self.xgb_classifier_path = xgb_classifier_path\n        self.xgb_rank_path = xgb_rank_path\n        self.rank_top_k = rank_top_k\n        self.lstm_param_path = lstm_param_path\n        self.predict_data_file = predict_data_file\n        self.result_save_path = result_save_path\n        if not os.path.exists(self.result_save_path):\n            os.mkdir(self.result_save_path)\n        \n        \n    def __call__(self, *args, **kwargs):\n        # 1. XGBoost 分类模型预测\n        pos_feature_data = self.xgb_classifier_predict()  # 利用 xgb 分类器预测获取阳性结果\n        xgboost_classify = pos_feature_data[\"sequence\"].values.tolist()\n        with open(os.path.join(self.result_save_path, \"xgb_classifier_pos_seq.txt\"), \"w\") as f:\n            for i in range(len(xgboost_classify)):\n                f.write(xgboost_classify[i])\n                f.write(\"\\n\")\n            f.close()\n        \n        # 2. XGBoost 排序模型预测\n        xgb_rank_model = joblib.load(self.xgb_rank_path)  # 获取 xgb 排序模型\n        xgb_rank_result = xgb_rank_model.predict(pos_feature_data.iloc[:, 1:-2].values)  # 利用 xgb 排序模型预测 MIC 值\n        dataframe = pd.DataFrame([])\n        dataframe[\"sequence\"] = pos_feature_data[\"sequence\"].copy()\n        dataframe[\"MIC\"] = xgb_rank_result\n        dataframe.sort_values(\"MIC\", inplace=True)\n        dataframe.reset_index(drop=True, inplace=True)\n        self.sequence = dataframe[\"sequence\"].values[0: int(self.rank_top_k)]\n        dataframe.iloc[:int(self.rank_top_k), :].to_csv(os.path.join(self.result_save_path, f\"top_{str(self.rank_top_k)}.csv\"))\n        \n        # 3. LSTM 回归模型预测\n        self.lstm_preidct()\n\n    def xgb_classifier_predict(self):\n        xgb_cls_model = joblib.load(self.xgb_classifier_path)\n        predict_data = pd.read_csv(self.predict_data_file, chunksize=50000, encoding=\"utf-8\", low_memory=False, index_col=0)\n        df = pd.DataFrame([])\n        for chunk in predict_data:\n            y = xgb_cls_model.predict(chunk.iloc[:, 0:-2].values)\n            mask = [bool(x) for x in y]  # 预测值y转为bool值，0为False，1为True\n            data = chunk[mask]  # 筛选出阳性结果\n            df = pd.concat([df, data])  # 合并所有阳性结果\n            print(df.describe())  # 统计每列的平均值、标准差等数据\n        df.reset_index(drop=True, inplace=True)  # 重置索引\n        df.to_csv(os.path.join(self.result_save_path, \"xgb_classifier_pos_data.csv\"), index=False)\n        return df \n\n\n    def lstm_preidct(self):\n        net = LstmNet(embedding_dim, hidden_num, num_layer, bidirectional, dropout, get_dict())\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        net.to(device)\n        net.eval()\n        # 构建数据集，dataset中每个单元由2个元素的元组构成，第一个是对序列的one-hot编码，第二个是序列的长度。\n        dataset = PredictDataset(self.sequence)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n        sequence = []\n        predict = []\n        for i, (text, length) in enumerate(dataloader):\n            text = text.to(device)\n            out = net(text, length).reshape(-1)\n            predict.extend(out.cpu().detach().numpy())\n            for i in text:\n                temp = ''.join([get_reverse_dict()[n] for n in i.tolist() if n > 0])\n                sequence.append(temp)\n        df = pd.DataFrame({\"sequence\": sequence, \"predict\": predict})\n        df.sort_values(\"predict\", inplace=True)\n        df.to_csv(os.path.join(self.lstm_result_save_path, \"lstm_regression_result.csv\"), index=False)","metadata":{},"outputs":[]},{"cell_type":"code","source":"predictor = Predict(xgb_classifier_path = \"xgb_classifier_model.pkl\",\n                    xgb_rank_path = \"xgb_rank_model.pkl\",\n                    rank_top_k = 500,\n                    lstm_param_path = \"pos_lstm_regress_model.pth\",\n                    predict_data_file = \"7_peptide_rule_0.csv\",\n                    result_save_path = \"results\")","metadata":{},"outputs":[]}]}